{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"tEKS \u00b6 tEKS is a set of Terraform / Terragrunt modules designed to get you everything you need to run a production EKS cluster on AWS. It ships with sensible defaults, and add a lot of common addons with their configurations that work out of the box. Main features \u00b6 Node pools with customizable labels / taints Fully customizable kubelet args Supports new or existing VPC Calico for network policies Common addons with associated IAM permissions if needed: cluster-autoscaler : scale worker nodes based on workload external-dns : sync ingress and service records in route53 cert-manager : automatically generate TLS certificates, supports ACME v2 kiam : prevents pods to access EC2 metadata and enables pods to assume specific AWS IAM roles nginx-ingress : processes Ingress object and acts as a HTTP/HTTPS proxy (compatible with cert-manager) metrics-server : enable metrics API and horizontal pod scaling (HPA) prometheus-operator : Monitoring / Alerting / Dashboards virtual-kubelet : enables using ECS Fargate as a provider to run workload without EC2 instances fluentd-cloudwatch : forwards logs to AWS Cloudwatch node-problem-detector : Forwards node problems to Kubernetes events Requirements \u00b6 Terraform Terragrunt kubectl helm Documentation \u00b6 User guides, feature documentation and examples are available here About Kiam \u00b6 Kiam prevents pods from accessing EC2 instances IAM role and therefore using the instances role to perform actions on AWS. It also allows pods to assume specific IAM roles if needed. To do so kiam-agent acts as an iptables proxy on nodes. It intercepts requests made to EC2 metadata and redirect them to a kiam-server that fetches IAM credentials and pass them to pods. For security reasons, because Kiam needs to assume an IAM role that can assume other roles, it is best to run it on a dedicated node with specific IAM permission where no other workload are running and where there is no kiam-agent (because kiam-server need access to EC2 metadata). This is taken care of by default but it is customizable. Addons that require specific IAM permissions \u00b6 Some addons interface with AWS API, for example: cluster-autoscaler external-dns cert-manager virtual-kubelet : only with Kiam enable Without KIAM \u00b6 If you are not using Kiam, addons must access EC2 instances metdata to get credentials and access Kubernetes API, it is best for security reason to use a dedicated node for addons in that case to avoid other pods to access IAM roles and messed up route53, or scale down your cluster for example. With Kiam \u00b6 If you are using Kiam, these addons can run either with instances IAM roles on the same dedicated nodes where kiam-server is running amd bypass kiam-agent or they can run anywhere and assume a specific role through Kiam. The following matrix tries to explain the possible combinations: virtual-kubelet (assume role with Kiam) cluster-autoscaler cluster-autoscaler cert-manager (assume role with Kiam) cert-manager (assume role with Kiam) external-dns external-dns cluster-autoscaler (assume role with Kiam) cluster-autoscaler (assume role with Kiam) cert-manager cert-manager virtual-kubelet (assume role with Kiam) external-dns (assume role with Kiam) external-dns (assume role with Kiam) kiam-server kiam-server kiam-agent kiam-server kiam-agent kiam-agent (in HostNetwork, bypass kiam-agent) Dedicated node(s) with IAM roles attached Worker node(s) Dedicated node(s) with IAM roles attached (bypass kiam-agent) Worker node(s) Dedicated node(s) with only IAM role for Kiam (bypass kiam-agent) Worker node(s) Worker node(s) with only IAM role for Kiam (bypass kiam-agent) !!! Security concerns !!! Without Kiam With Kiam With Kiam With Kiam License \u00b6","title":"Overview"},{"location":"#teks","text":"tEKS is a set of Terraform / Terragrunt modules designed to get you everything you need to run a production EKS cluster on AWS. It ships with sensible defaults, and add a lot of common addons with their configurations that work out of the box.","title":"tEKS"},{"location":"#main-features","text":"Node pools with customizable labels / taints Fully customizable kubelet args Supports new or existing VPC Calico for network policies Common addons with associated IAM permissions if needed: cluster-autoscaler : scale worker nodes based on workload external-dns : sync ingress and service records in route53 cert-manager : automatically generate TLS certificates, supports ACME v2 kiam : prevents pods to access EC2 metadata and enables pods to assume specific AWS IAM roles nginx-ingress : processes Ingress object and acts as a HTTP/HTTPS proxy (compatible with cert-manager) metrics-server : enable metrics API and horizontal pod scaling (HPA) prometheus-operator : Monitoring / Alerting / Dashboards virtual-kubelet : enables using ECS Fargate as a provider to run workload without EC2 instances fluentd-cloudwatch : forwards logs to AWS Cloudwatch node-problem-detector : Forwards node problems to Kubernetes events","title":"Main features"},{"location":"#requirements","text":"Terraform Terragrunt kubectl helm","title":"Requirements"},{"location":"#documentation","text":"User guides, feature documentation and examples are available here","title":"Documentation"},{"location":"#about-kiam","text":"Kiam prevents pods from accessing EC2 instances IAM role and therefore using the instances role to perform actions on AWS. It also allows pods to assume specific IAM roles if needed. To do so kiam-agent acts as an iptables proxy on nodes. It intercepts requests made to EC2 metadata and redirect them to a kiam-server that fetches IAM credentials and pass them to pods. For security reasons, because Kiam needs to assume an IAM role that can assume other roles, it is best to run it on a dedicated node with specific IAM permission where no other workload are running and where there is no kiam-agent (because kiam-server need access to EC2 metadata). This is taken care of by default but it is customizable.","title":"About Kiam"},{"location":"#addons-that-require-specific-iam-permissions","text":"Some addons interface with AWS API, for example: cluster-autoscaler external-dns cert-manager virtual-kubelet : only with Kiam enable","title":"Addons that require specific IAM permissions"},{"location":"#without-kiam","text":"If you are not using Kiam, addons must access EC2 instances metdata to get credentials and access Kubernetes API, it is best for security reason to use a dedicated node for addons in that case to avoid other pods to access IAM roles and messed up route53, or scale down your cluster for example.","title":"Without KIAM"},{"location":"#with-kiam","text":"If you are using Kiam, these addons can run either with instances IAM roles on the same dedicated nodes where kiam-server is running amd bypass kiam-agent or they can run anywhere and assume a specific role through Kiam. The following matrix tries to explain the possible combinations: virtual-kubelet (assume role with Kiam) cluster-autoscaler cluster-autoscaler cert-manager (assume role with Kiam) cert-manager (assume role with Kiam) external-dns external-dns cluster-autoscaler (assume role with Kiam) cluster-autoscaler (assume role with Kiam) cert-manager cert-manager virtual-kubelet (assume role with Kiam) external-dns (assume role with Kiam) external-dns (assume role with Kiam) kiam-server kiam-server kiam-agent kiam-server kiam-agent kiam-agent (in HostNetwork, bypass kiam-agent) Dedicated node(s) with IAM roles attached Worker node(s) Dedicated node(s) with IAM roles attached (bypass kiam-agent) Worker node(s) Dedicated node(s) with only IAM role for Kiam (bypass kiam-agent) Worker node(s) Worker node(s) with only IAM role for Kiam (bypass kiam-agent) !!! Security concerns !!! Without Kiam With Kiam With Kiam With Kiam","title":"With Kiam"},{"location":"#license","text":"","title":"License"},{"location":"user-guides/","text":"Deploying tEKS from scratch \u00b6 Requirements \u00b6 The following dependencies are required on the deployer host: Terraform Terragrunt kubectl helm aws-iam-authenticator Deployment type \u00b6 tEKS is customizable, the following guides are available: Minimal installation Minimal installation with kiam Full installation Full installation with Kiam","title":"Getting Started"},{"location":"user-guides/#deploying-teks-from-scratch","text":"","title":"Deploying tEKS from scratch"},{"location":"user-guides/#requirements","text":"The following dependencies are required on the deployer host: Terraform Terragrunt kubectl helm aws-iam-authenticator","title":"Requirements"},{"location":"user-guides/#deployment-type","text":"tEKS is customizable, the following guides are available: Minimal installation Minimal installation with kiam Full installation Full installation with Kiam","title":"Deployment type"},{"location":"user-guides/minimal-deployment/","text":"Deploying tEKS minimal version \u00b6 This guide explains how to deploy a minimal version of tEKS that's basically a bare EKS cluster. Prepping environment \u00b6 Terragrunt environment \u00b6 In terraform/live folder, copy the sample environment to a new folder, we will use minimal across this guide: cp -ar sample minimal tree . \u251c\u2500\u2500 minimal \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u2514\u2500\u2500 terraform.tfvars \u2502 \u2514\u2500\u2500 eks-addons \u2502 \u2514\u2500\u2500 terraform.tfvars \u251c\u2500\u2500 sample \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u2514\u2500\u2500 terraform.tfvars \u2502 \u2514\u2500\u2500 eks-addons \u2502 \u2514\u2500\u2500 terraform.tfvars \u2514\u2500\u2500 terraform.tfvars 6 directories, 5 files Terragrunt remote state \u00b6 Edit live/terraform.tfvars : terragrunt = { remote_state { backend = \"s3\" config { bucket = \"sample-terraform-remote-state\" key = \"${path_relative_to_include()}\" region = \"eu-west-1\" encrypt = true dynamodb_table = \"sample-terraform-remote-state\" } } } Change bucket and dynamodb to suit your environment, Terragrunt can create the bucket and dynamodb table if they do not exist. EKS module variables \u00b6 Edit live/minimal/eks/terraform.tfvars : This module setup infrastructure components and everything related to AWS, such as IAM permission if necessary. Everything should already be turned off by default. You should just have to edit cluster-name and the aws[\"region\"] variable. You also need to customize the node pool at the end of the file to suit your needs. You need to, at least, change the SSH Key for one available in S3. node-pools = [ { name = \"controller\" min_size = 1 max_size = 1 desired_capacity = 1 instance_type = \"t3.medium\" key_name = \"klefevre-sorrow\" volume_size = 30 volume_type = \"gp2\" autoscaling = \"disabled\" kubelet_extra_args = \"--kubelet-extra-args '--node-labels node-role.kubernetes.io/controller=\\\"\\\" --register-with-taints node-role.kubernetes.io/controller=:NoSchedule --kube-reserved cpu=250m,memory=0.5Gi --system-reserved cpu=250m,memory=0.2Gi,ephemeral-storage=1Gi,ephemeral-storage=1Gi --eviction-hard memory.available<500Mi,nodefs.available<10%'\" }, { name = \"default\" min_size = 3 max_size = 9 desired_capacity = 3 instance_type = \"t3.medium\" key_name = \"klefevre-sorrow\" volume_size = 30 volume_type = \"gp2\" autoscaling = \"enabled\" kubelet_extra_args = \"--kubelet-extra-args '--node-labels node-role.kubernetes.io/node=\\\"\\\" --kube-reserved cpu=250m,memory=0.5Gi --system-reserved cpu=250m,memory=0.2Gi,ephemeral-storage=1Gi,ephemeral-storage=1Gi --eviction-hard memory.available<500Mi,nodefs.available<10%'\" }, ] EKS addons module \u00b6 By default everything is disabled for this module so no particular changes are required except for this part: eks = { \"kubeconfig_path\" = \"./kubeconfig\" \"remote_state_bucket\" = \"sample-terraform-remote-state\" \"remote_state_key\" = \"sample/eks\" } This part should reflect your environment with your S3 bucket where the state fils are stored and also the key which is equivalent to the folder where the modules variables are defined, in our case minimal/eks . Planning the deployment \u00b6 In the minimal folder: terragrunt plan-all terragrunt apply-all Once completed, you should be able to access cluster: export KUBECONFIG = $( pwd ) /eks/kubeconfig kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-29-11.eu-west-1.compute.internal Ready node 2d v1.11.5 ip-10-0-49-90.eu-west-1.compute.internal Ready node 2d v1.11.5 ip-10-0-59-209.eu-west-1.compute.internal Ready controller 2d v1.11.5 ip-10-0-85-237.eu-west-1.compute.internal Ready node 2d v1.11.5","title":"Minimal"},{"location":"user-guides/minimal-deployment/#deploying-teks-minimal-version","text":"This guide explains how to deploy a minimal version of tEKS that's basically a bare EKS cluster.","title":"Deploying tEKS minimal version"},{"location":"user-guides/minimal-deployment/#prepping-environment","text":"","title":"Prepping environment"},{"location":"user-guides/minimal-deployment/#terragrunt-environment","text":"In terraform/live folder, copy the sample environment to a new folder, we will use minimal across this guide: cp -ar sample minimal tree . \u251c\u2500\u2500 minimal \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u2514\u2500\u2500 terraform.tfvars \u2502 \u2514\u2500\u2500 eks-addons \u2502 \u2514\u2500\u2500 terraform.tfvars \u251c\u2500\u2500 sample \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u2514\u2500\u2500 terraform.tfvars \u2502 \u2514\u2500\u2500 eks-addons \u2502 \u2514\u2500\u2500 terraform.tfvars \u2514\u2500\u2500 terraform.tfvars 6 directories, 5 files","title":"Terragrunt environment"},{"location":"user-guides/minimal-deployment/#terragrunt-remote-state","text":"Edit live/terraform.tfvars : terragrunt = { remote_state { backend = \"s3\" config { bucket = \"sample-terraform-remote-state\" key = \"${path_relative_to_include()}\" region = \"eu-west-1\" encrypt = true dynamodb_table = \"sample-terraform-remote-state\" } } } Change bucket and dynamodb to suit your environment, Terragrunt can create the bucket and dynamodb table if they do not exist.","title":"Terragrunt remote state"},{"location":"user-guides/minimal-deployment/#eks-module-variables","text":"Edit live/minimal/eks/terraform.tfvars : This module setup infrastructure components and everything related to AWS, such as IAM permission if necessary. Everything should already be turned off by default. You should just have to edit cluster-name and the aws[\"region\"] variable. You also need to customize the node pool at the end of the file to suit your needs. You need to, at least, change the SSH Key for one available in S3. node-pools = [ { name = \"controller\" min_size = 1 max_size = 1 desired_capacity = 1 instance_type = \"t3.medium\" key_name = \"klefevre-sorrow\" volume_size = 30 volume_type = \"gp2\" autoscaling = \"disabled\" kubelet_extra_args = \"--kubelet-extra-args '--node-labels node-role.kubernetes.io/controller=\\\"\\\" --register-with-taints node-role.kubernetes.io/controller=:NoSchedule --kube-reserved cpu=250m,memory=0.5Gi --system-reserved cpu=250m,memory=0.2Gi,ephemeral-storage=1Gi,ephemeral-storage=1Gi --eviction-hard memory.available<500Mi,nodefs.available<10%'\" }, { name = \"default\" min_size = 3 max_size = 9 desired_capacity = 3 instance_type = \"t3.medium\" key_name = \"klefevre-sorrow\" volume_size = 30 volume_type = \"gp2\" autoscaling = \"enabled\" kubelet_extra_args = \"--kubelet-extra-args '--node-labels node-role.kubernetes.io/node=\\\"\\\" --kube-reserved cpu=250m,memory=0.5Gi --system-reserved cpu=250m,memory=0.2Gi,ephemeral-storage=1Gi,ephemeral-storage=1Gi --eviction-hard memory.available<500Mi,nodefs.available<10%'\" }, ]","title":"EKS module variables"},{"location":"user-guides/minimal-deployment/#eks-addons-module","text":"By default everything is disabled for this module so no particular changes are required except for this part: eks = { \"kubeconfig_path\" = \"./kubeconfig\" \"remote_state_bucket\" = \"sample-terraform-remote-state\" \"remote_state_key\" = \"sample/eks\" } This part should reflect your environment with your S3 bucket where the state fils are stored and also the key which is equivalent to the folder where the modules variables are defined, in our case minimal/eks .","title":"EKS addons module"},{"location":"user-guides/minimal-deployment/#planning-the-deployment","text":"In the minimal folder: terragrunt plan-all terragrunt apply-all Once completed, you should be able to access cluster: export KUBECONFIG = $( pwd ) /eks/kubeconfig kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-29-11.eu-west-1.compute.internal Ready node 2d v1.11.5 ip-10-0-49-90.eu-west-1.compute.internal Ready node 2d v1.11.5 ip-10-0-59-209.eu-west-1.compute.internal Ready controller 2d v1.11.5 ip-10-0-85-237.eu-west-1.compute.internal Ready node 2d v1.11.5","title":"Planning the deployment"}]}